{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "%matplotlib inline\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xavier_initializer(n_inputs, n_outputs, uniform = True):\n",
    "    if uniform:\n",
    "        init_range = tf.sqrt(6.0/ (n_inputs + n_outputs))\n",
    "        return tf.random_uniform_initializer(-init_range, init_range)\n",
    "\n",
    "    else:\n",
    "        stddev = tf.sqrt(3.0 / (n_inputs + n_outputs))\n",
    "        return tf.truncated_normal_initializer(stddev=stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, sess, name, input_dim=784, output_dim=10, hidden_dims=[500], activation_fn=tf.nn.relu, optimizer=tf.train.AdamOptimizer, lr=0.01):\n",
    "        self.sess = sess\n",
    "        with tf.variable_scope(name):\n",
    "            self.X = tf.placeholder(tf.float32, [None, input_dim], name='X')\n",
    "            self.Y = tf.placeholder(tf.float32, [None, output_dim], name = 'Y')\n",
    "            self.mode = tf.placeholder(tf.bool, name='train_mode')\n",
    "            \n",
    "            current_dim = input_dim\n",
    "            current_layer = self.X\n",
    "            \n",
    "            for i, h_dim in enumerate(hidden_dims):\n",
    "                with tf.variable_scope('layer{}'.format(i)):\n",
    "                    W = tf.get_variable(shape=[current_dim, h_dim], name='weight', initializer=xavier_initializer(current_dim, h_dim))\n",
    "                    b = tf.Variable(tf.zeros([h_dim]), name='bias')\n",
    "                    layer = tf.matmul(current_layer, W) + b    \n",
    "                    current_layer = activation_fn(layer)\n",
    "                    current_dim = h_dim\n",
    "            \n",
    "            with tf.variable_scope('final_layer'):\n",
    "                W = tf.get_variable(shape=[current_dim, output_dim], name='weight', initializer=xavier_initializer(current_dim, output_dim))\n",
    "                b = tf.Variable(tf.zeros([output_dim]), name='bias')\n",
    "                current_layer = tf.matmul(current_layer, W) + b\n",
    "            \n",
    "            self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=current_layer, labels = self.Y)\n",
    "            self.loss = tf.reduce_mean(self.loss, name='loss')\n",
    "            self.train_op = optimizer(lr).minimize(self.loss)        \n",
    "        \n",
    "            softmax = tf.nn.softmax(current_layer, name='softmax')\n",
    "            self.accuracy = tf.equal(tf.argmax(softmax, 1), tf.argmax(self.Y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.accuracy, tf.float32))\n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    \n",
    "    def train(self,X,Y):\n",
    "        feed = {\n",
    "            self.X:X,\n",
    "            self.Y:Y,\n",
    "            self.mode: True\n",
    "        }\n",
    "        return self.sess.run([self.train_op, self.loss], feed_dict=feed)\n",
    "    \n",
    "    \n",
    "    def evaluate(self, X, Y, batch_size=None):\n",
    "        feed = {\n",
    "            self.X: X,\n",
    "            self.Y: Y,\n",
    "            self.mode: False\n",
    "        }\n",
    "        return self.sess.run([self.loss, self.accuracy], feed_dict=feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAINING_EPOCHS = 10\n",
    "BATCH_SIZE = 500\n",
    "N = 55000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = NN(sess,'first_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost: 0.296566475589\n",
      "Precision: 0.9649\n",
      "Epoch: 2 cost: 0.0932131653821\n",
      "Precision: 0.9701\n",
      "Epoch: 3 cost: 0.0609748106958\n",
      "Precision: 0.976\n",
      "Epoch: 4 cost: 0.0440022152425\n",
      "Precision: 0.9741\n",
      "Epoch: 5 cost: 0.0356760977937\n",
      "Precision: 0.9763\n",
      "Epoch: 6 cost: 0.0285113705788\n",
      "Precision: 0.9762\n",
      "Epoch: 7 cost: 0.0238379813773\n",
      "Precision: 0.978\n",
      "Epoch: 8 cost: 0.0204468847803\n",
      "Precision: 0.9777\n",
      "Epoch: 9 cost: 0.0193043943198\n",
      "Precision: 0.974\n",
      "Epoch: 10 cost: 0.0202411584006\n",
      "Precision: 0.9782\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(TRAINING_EPOCHS):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / BATCH_SIZE)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "        _, c = model.train(batch_xs, batch_ys)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch: ' + str((epoch + 1)) + ' cost: ' + str((avg_cost)))\n",
    "    l, p = model.evaluate(mnist.test.images, mnist.test.labels)\n",
    "    \n",
    "    print('Precision: ' + str(p))\n",
    "\n",
    "print('Learning Finished!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
